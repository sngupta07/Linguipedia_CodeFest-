{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Competition organised by [AnalyticsVidhya](http://analyticsvidhya.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "The problem is based on the sentiment analysis, we have been given the reviews based on products and we have to predict the its sentiment. Sentiment is classified in two parts 0 and 1 (0 negative and 1 positive).\n",
    "\n",
    "### Datasets\n",
    "The dataset contains two features \n",
    "1. Label (0 and 1)\n",
    "2. Tweets\n",
    "\n",
    "We have provided three dataset train, test and sample submission.\n",
    "\n",
    "### Metrics used\n",
    "Here, we have to use f1-score to compete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize']= (15, 12)\n",
    "plt.style.use('ggplot')\n",
    "sns.set(color_codes= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv('train.csv')\n",
    "df_test= pd.read_csv('test.csv')\n",
    "df_samp= pd.read_csv('sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing_text_length(tweets):\n",
    "    num_words= []\n",
    "    num_char= []\n",
    "    avg_word_len= []\n",
    "    num_stopwords= []\n",
    "    num_special_char= []\n",
    "    num_upper_cases= []\n",
    "    num_numerics= []\n",
    "\n",
    "    for tweet in tweets:\n",
    "        #remove links \n",
    "        #tweet= re.sub(r'(http|https|ftp)://[a-zA-Z0-9\\./]+', '', tweet, flags= re.I)\n",
    "        #tweet = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', tweet, flags=re.MULTILINE)\n",
    "        \n",
    "        tweet= tweet.strip().split()\n",
    "        \n",
    "        #num of words\n",
    "        words= [w for w in tweet]\n",
    "        num_words.append(len(words))\n",
    "        \n",
    "        #num_char\n",
    "        chars= len(tweet)\n",
    "        num_char.append(chars)\n",
    "        \n",
    "        #num_avg word length\n",
    "        words_avg_len= [sum(len(w) for w in tweet)/len(words)]\n",
    "        avg_word_len.append(words_avg_len[0])\n",
    "        \n",
    "        #number of stop words\n",
    "        from nltk.corpus import stopwords\n",
    "        stop= stopwords.words('english')\n",
    "        stopword= [w for w in tweet if w in stop]\n",
    "        num_stopwords.append(len(stopword))\n",
    "        \n",
    "        #number of special character\n",
    "        hastags= [w for w in tweet if w.startswith('#')]\n",
    "        num_special_char.append(len(hastags))\n",
    "        \n",
    "        #number of numerics\n",
    "        numerics= [w for w in tweet if w.isdigit()]\n",
    "        num_numerics.append(len(numerics))\n",
    "        \n",
    "        #number of upper cases\n",
    "        upper_cases= [w for w in tweet if w.isupper()]\n",
    "        num_upper_cases.append(len(upper_cases))\n",
    "        \n",
    "    return num_char, num_numerics, num_special_char, num_stopwords, num_upper_cases, num_words, avg_word_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all= pd.concat([df_train, df_test])\n",
    "\n",
    "num_chars, num_numeric, num_special_chars, num_stopword, num_upper_case, num_word, avg_words_len= preprocessing_text_length(df_all['tweet'])\n",
    "\n",
    "df_all['num_chars']= num_chars\n",
    "df_all['num_numeric']= num_numeric\n",
    "df_all['num_special_chars']= num_special_chars\n",
    "df_all['num_stopword']= num_stopword\n",
    "df_all['num_upper_case']= num_upper_case\n",
    "df_all['num_word']= num_word\n",
    "df_all['avg_words_len']= avg_words_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocessing(tweets):\n",
    "    \n",
    "    #remove punctuations\n",
    "    tweets= tweets.str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    #remove stopwords\n",
    "    from nltk.corpus import stopwords\n",
    "    stop= stopwords.words('english')\n",
    "    \n",
    "    tweets= tweets.apply(lambda x: ' '.join(w for w in x.split() if w not in stop))\n",
    "    \n",
    "    #common word removal\n",
    "    freq= pd.Series(' '.join(tweets).split()).value_counts()[:10]\n",
    "    \n",
    "    freq= list(freq)\n",
    "    tweets= tweets.apply(lambda x: ' '.join(w for w in x.split() if w not in freq))\n",
    "    \n",
    "    #rare word removal\n",
    "    freq= pd.Series(' '.join(tweets).split()).value_counts()[-10:]\n",
    "    \n",
    "    freq= list(freq)\n",
    "    tweets= tweets.apply(lambda x: ' '.join(w for w in x.split() if w not in freq))\n",
    "    \n",
    "    #spelling correction\n",
    "    #from textblob import TextBlob\n",
    "    #tweets= tweets.apply(lambda x: str(TextBlob(x).correct()))\n",
    "    \n",
    "    #tokenize\n",
    "    from nltk.tokenize import TreebankWordTokenizer\n",
    "    tokenizer= TreebankWordTokenizer()\n",
    "    #tweets= tweets.apply(lambda x: (' '.join(w) for w tokenizer.tokenize(x)))\n",
    "    \n",
    "    #stemming\n",
    "    #from nltk.stem import PorterStemmer\n",
    "    #stemmer= PorterStemmer()\n",
    "    #tweets= tweets.apply(lambda x: ' '.join[stemmer.stem(w for w in tokenizer.tokenize(x))])\n",
    "    \n",
    "    #lemmatization\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer= WordNetLemmatizer()\n",
    "    #tweets= tweets.apply(lambda x: lemmatizer.lemmatize(w) for w in x)\n",
    "    tweets= tweets.apply(lambda x: tokenizer.tokenize(x))\n",
    "    tweets= tweets.apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "    tweets= tweets.apply(lambda x: ' '.join(w for w in x))\n",
    "    \n",
    "    #convert it into lower case\n",
    "    tweets= tweets.apply(lambda tweet: tweet.strip().lower())\n",
    "                                             \n",
    "    return tweets\n",
    "\n",
    "tweets= df_all['tweet']\n",
    "tweets= basic_preprocessing(tweets)\n",
    "\n",
    "df_all['tweets']= tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_numeric</th>\n",
       "      <th>num_special_chars</th>\n",
       "      <th>num_stopword</th>\n",
       "      <th>num_upper_case</th>\n",
       "      <th>num_word</th>\n",
       "      <th>avg_words_len</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#fingerprint #Pregnancy Test https://goo.gl/h1...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>8.923077</td>\n",
       "      <td>fingerprint pregnancy test httpsgooglh1mfqv an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Finally a transparant silicon case ^^ Thanks t...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>6.764706</td>\n",
       "      <td>finally transparant silicon case thanks uncle ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>We love this! Would you go? #talk #makememorie...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>7.266667</td>\n",
       "      <td>we love would go talk makememories unplug rela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'm wired I know I'm George I was made that wa...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>5.647059</td>\n",
       "      <td>im wired i know im george i made way iphone cu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>What amazing service! Apple won't even talk to...</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>4.434783</td>\n",
       "      <td>what amazing service apple wont even talk ques...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  num_chars  \\\n",
       "0   1    0.0  #fingerprint #Pregnancy Test https://goo.gl/h1...         13   \n",
       "1   2    0.0  Finally a transparant silicon case ^^ Thanks t...         17   \n",
       "2   3    0.0  We love this! Would you go? #talk #makememorie...         15   \n",
       "3   4    0.0  I'm wired I know I'm George I was made that wa...         17   \n",
       "4   5    1.0  What amazing service! Apple won't even talk to...         23   \n",
       "\n",
       "   num_numeric  num_special_chars  num_stopword  num_upper_case  num_word  \\\n",
       "0            0                 11             0               0        13   \n",
       "1            0                  5             3               1        17   \n",
       "2            0                  8             1               0        15   \n",
       "3            0                  4             2               2        17   \n",
       "4            0                  0             9               2        23   \n",
       "\n",
       "   avg_words_len                                             tweets  \n",
       "0       8.923077  fingerprint pregnancy test httpsgooglh1mfqv an...  \n",
       "1       6.764706  finally transparant silicon case thanks uncle ...  \n",
       "2       7.266667  we love would go talk makememories unplug rela...  \n",
       "3       5.647059  im wired i know im george i made way iphone cu...  \n",
       "4       4.434783  what amazing service apple wont even talk ques...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= df_all[:len(df_train)]\n",
    "test= df_all[len(df_train):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack  ##it is used to add the columns in sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_add= train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_add.remove('id')\n",
    "col_to_add.remove('label')\n",
    "col_to_add.remove('tweet')\n",
    "col_to_add.remove('tweets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applied machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_modeling(model, train, target, test, countVectorizer= True, tfidfVectorizer= False, col_to_add= col_to_add):\n",
    "    \n",
    "    #splitted the dataset in training and validation set (splitted at 25%)\n",
    "    X_train, X_test, y_train, y_test= train_test_split(train, target, random_state= 5) \n",
    "    \n",
    "    #feature extraction techniques (count vectorizer and tfidf vectorizer)\n",
    "    if countVectorizer:  \n",
    "        vect= CountVectorizer().fit(X_train['tweets'])\n",
    "        X_train_vect= vect.transform(X_train['tweets'])\n",
    "        X_test_vect= vect.transform(X_test['tweets'])\n",
    "        test_vect= vect.transform(test['tweets'])\n",
    "        \n",
    "        #now add the columns to the sparse matrix using the scipy library\n",
    "        for col in col_to_add:\n",
    "            X_train_vect= hstack((X_train_vect, np.array(X_train[col])[:, None]))\n",
    "            X_test_vect= hstack((X_test_vect, np.array(X_test[col])[:, None]))\n",
    "            test_vect= hstack((test_vect, np.array(test[col])[:, None]))\n",
    "        \n",
    "        print('X_train_vect', X_train_vect.shape)\n",
    "        print('X_test_vect', X_test_vect.shape)\n",
    "        print('test_vect', test_vect.shape)\n",
    "        \n",
    "        #modeling and prediction\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        prediction= model.predict(X_test_vect)\n",
    "        \n",
    "        #print accuracies\n",
    "        train_acc= model.score(X_train_vect, y_train)\n",
    "        test_acc= accuracy_score(y_test, prediction)\n",
    "        f1= f1_score(y_test, prediction)\n",
    "        \n",
    "        print('Training accuracy: {}' .format(train_acc))\n",
    "        print('Testing accuracy: {}' .format(test_acc))\n",
    "        print('f1 score: {}' .format(f1))\n",
    "        \n",
    "        print('Classification Report: ')\n",
    "        print(classification_report(y_test, prediction))\n",
    "        \n",
    "    if tfidfVectorizer:\n",
    "        vect= TfidfVectorizer(min_df= 5, ngram_range= (1, 3)).fit(X_train['tweets'])\n",
    "        X_train_vect= vect.transform(X_train['tweets'])\n",
    "        X_test_vect= vect.transform(X_test['tweets'])\n",
    "        test_vect= vect.transform(test['tweets'])\n",
    "        \n",
    "        #now add the columns to the sparse matrix using the scipy library\n",
    "        for col in col_to_add:\n",
    "            X_train_vect= hstack((X_train_vect, np.array(X_train[col])[:, None]))\n",
    "            X_test_vect= hstack((X_test_vect, np.array(X_test[col])[:, None]))\n",
    "            test_vect= hstack((test_vect, np.array(test[col])[:, None]))\n",
    "        \n",
    "        print('X_train_vect', X_train_vect.shape)\n",
    "        print('X_test_vect', X_test_vect.shape)\n",
    "        print('test_vect', test_vect.shape)\n",
    "        \n",
    "        #modeling and prediction\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        prediction= model.predict(X_test_vect)\n",
    "        \n",
    "        #print accuracies\n",
    "        train_acc= model.score(X_train_vect, y_train)\n",
    "        test_acc= accuracy_score(y_test, prediction)\n",
    "        f1= f1_score(y_test, prediction)\n",
    "        \n",
    "        print('Training accuracy: {}' .format(train_acc))\n",
    "        print('Testing accuracy: {}' .format(test_acc))\n",
    "        print('f1 score: {}' .format(f1))\n",
    "        \n",
    "        print('Classification Report: ')\n",
    "        print(classification_report(y_test, prediction))\n",
    "        \n",
    "    return model, model.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_log= LogisticRegression(class_weight= 'balanced', random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 18026)\n",
      "X_test_vect (1980, 18026)\n",
      "test_vect (1953, 18026)\n",
      "Training accuracy: 0.9683501683501684\n",
      "Testing accuracy: 0.8984848484848484\n",
      "f1 score: 0.8157653528872594\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.90      0.93      1475\n",
      "        1.0       0.76      0.88      0.82       505\n",
      "\n",
      "avg / total       0.91      0.90      0.90      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using count vectorizer\n",
    "model_1, pred_1= ml_modeling(clf_log, train.drop(['id', 'tweet', 'label'], axis= 1), train['label'],\n",
    "                             test.drop(['id', 'tweet', 'label'], axis= 1)) ## called the ml function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sngupta\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 3189)\n",
      "X_test_vect (1980, 3189)\n",
      "test_vect (1953, 3189)\n",
      "Training accuracy: 0.9104377104377105\n",
      "Testing accuracy: 0.8909090909090909\n",
      "f1 score: 0.8128249566724437\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.97      0.88      0.92      1475\n",
      "        1.0       0.72      0.93      0.81       505\n",
      "\n",
      "avg / total       0.91      0.89      0.89      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using tfidf vectorizer\n",
    "model_2, pred_2= ml_modeling(clf_log, train.drop(['id', 'tweet', 'label'], axis= 1), train['label'],\n",
    "                             test.drop(['id', 'tweet', 'label'], axis= 1), False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This function directly return the tweets after vecorize it, there in no need to write it again and again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer(train, target, test, countVectorizer= True, tfidfVectorizer= False, col_to_add= col_to_add):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test= train_test_split(train, target, random_state= 5)\n",
    "    \n",
    "    if countVectorizer:\n",
    "        vect= CountVectorizer().fit(X_train['tweets'])\n",
    "        X_train_vect= vect.transform(X_train['tweets'])\n",
    "        X_test_vect= vect.transform(X_test['tweets'])\n",
    "        test_vect= vect.transform(test['tweets'])\n",
    "        \n",
    "        #now add the columns to the sparse matrix using the scipy library\n",
    "        for col in col_to_add:\n",
    "            X_train_vect= hstack((X_train_vect, np.array(X_train[col])[:, None]))\n",
    "            X_test_vect= hstack((X_test_vect, np.array(X_test[col])[:, None]))\n",
    "            test_vect= hstack((test_vect, np.array(test[col])[:, None]))\n",
    "        \n",
    "        print('X_train_vect', X_train_vect.shape)\n",
    "        print('X_test_vect', X_test_vect.shape)\n",
    "        print('test_vect', test_vect.shape)\n",
    "        \n",
    "    if tfidfVectorizer:\n",
    "        vect= TfidfVectorizer(min_df= 5, ngram_range= (1, 3)).fit(X_train['tweets'])\n",
    "        X_train_vect= vect.transform(X_train['tweets'])\n",
    "        X_test_vect= vect.transform(X_test['tweets'])\n",
    "        test_vect= vect.transform(test['tweets'])\n",
    "        \n",
    "        #now add the columns to the sparse matrix using the scipy library\n",
    "        for col in col_to_add:\n",
    "            X_train_vect= hstack((X_train_vect, np.array(X_train[col])[:, None]))\n",
    "            X_test_vect= hstack((X_test_vect, np.array(X_test[col])[:, None]))\n",
    "            test_vect= hstack((test_vect, np.array(test[col])[:, None]))\n",
    "        \n",
    "        print('X_train_vect', X_train_vect.shape)\n",
    "        print('X_test_vect', X_test_vect.shape)\n",
    "        print('test_vect', test_vect.shape)\n",
    "        \n",
    "    return X_train_vect, X_test_vect, test_vect, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 18026)\n",
      "X_test_vect (1980, 18026)\n",
      "test_vect (1953, 18026)\n"
     ]
    }
   ],
   "source": [
    "##using count vectorizer\n",
    "X_train_count, X_test_count, test_count, y_train_c, y_test_c= vectorizer(train.drop(['id', 'tweet', 'label'], axis= 1), train['label'],\n",
    "                                                    test.drop(['id', 'tweet', 'label'], axis= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sngupta\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 3189)\n",
      "X_test_vect (1980, 3189)\n",
      "test_vect (1953, 3189)\n"
     ]
    }
   ],
   "source": [
    "##using tfidf vectorizer\n",
    "X_train_tfidf, X_test_tfidf, test_tfidf, y_train_t, y_test_t= vectorizer(train.drop(['id', 'tweet', 'label'], axis= 1), train['label'],\n",
    "                                                    test.drop(['id', 'tweet', 'label'], axis= 1), False, tfidfVectorizer= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5940, 18026), (1980, 18026), (1953, 18026), (5940,), (1980,))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape, X_test_count.shape, test_count.shape, y_train_c.shape, y_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5940, 3189), (1980, 3189), (1953, 3189), (5940,), (1980,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.shape, X_test_tfidf.shape, test_tfidf.shape, y_train_c.shape, y_test_t.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, here, I'm going to implement the ANN model for the sake of getting the better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies related to deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LeakyReLU, Activation\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy, sparse_categorical_crossentropy\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make data for deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csr import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count= csr_matrix(X_train_count)\n",
    "X_test_count= csr_matrix(X_test_count)\n",
    "test_count= csr_matrix(test_count)\n",
    "#X_test_count_1= X_test_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model declaration\n",
    "Here, I used\n",
    "1. Used 3 hidden layer\n",
    "2. Used hidden nodes (100, 50, 20)\n",
    "3. Used ReLu and sigmoid activation function\n",
    "4. Used stochastic gradient decent optimizer\n",
    "5. Used binary cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5940 samples, validate on 1980 samples\n",
      "Epoch 1/100\n",
      "5940/5940 [==============================] - 5s 788us/step - loss: 0.4670 - acc: 0.7675 - val_loss: 0.3579 - val_acc: 0.8359\n",
      "Epoch 2/100\n",
      "5940/5940 [==============================] - 4s 618us/step - loss: 0.3573 - acc: 0.8370 - val_loss: 0.3396 - val_acc: 0.8434\n",
      "Epoch 3/100\n",
      "5940/5940 [==============================] - 4s 595us/step - loss: 0.3439 - acc: 0.8426 - val_loss: 0.3332 - val_acc: 0.8455\n",
      "Epoch 4/100\n",
      "5940/5940 [==============================] - 4s 608us/step - loss: 0.3385 - acc: 0.8492 - val_loss: 0.3298 - val_acc: 0.8470\n",
      "Epoch 5/100\n",
      "5940/5940 [==============================] - 4s 664us/step - loss: 0.3348 - acc: 0.8510 - val_loss: 0.3266 - val_acc: 0.8530\n",
      "Epoch 6/100\n",
      "5940/5940 [==============================] - 4s 721us/step - loss: 0.3317 - acc: 0.8517 - val_loss: 0.3240 - val_acc: 0.8551\n",
      "Epoch 7/100\n",
      "5940/5940 [==============================] - 4s 619us/step - loss: 0.3290 - acc: 0.8535 - val_loss: 0.3219 - val_acc: 0.8576\n",
      "Epoch 8/100\n",
      "5940/5940 [==============================] - 4s 634us/step - loss: 0.3270 - acc: 0.8544 - val_loss: 0.3212 - val_acc: 0.8591\n",
      "Epoch 9/100\n",
      "5940/5940 [==============================] - 4s 696us/step - loss: 0.3254 - acc: 0.8542 - val_loss: 0.3177 - val_acc: 0.8621\n",
      "Epoch 10/100\n",
      "5940/5940 [==============================] - 4s 681us/step - loss: 0.3238 - acc: 0.8544 - val_loss: 0.3198 - val_acc: 0.8611\n",
      "Epoch 11/100\n",
      "5940/5940 [==============================] - 4s 597us/step - loss: 0.3219 - acc: 0.8542 - val_loss: 0.3163 - val_acc: 0.8626\n",
      "Epoch 12/100\n",
      "5940/5940 [==============================] - 3s 581us/step - loss: 0.3202 - acc: 0.8556 - val_loss: 0.3124 - val_acc: 0.8646\n",
      "Epoch 13/100\n",
      "5940/5940 [==============================] - 4s 643us/step - loss: 0.3182 - acc: 0.8566 - val_loss: 0.3136 - val_acc: 0.8636\n",
      "Epoch 14/100\n",
      "5940/5940 [==============================] - 4s 645us/step - loss: 0.3172 - acc: 0.8567 - val_loss: 0.3105 - val_acc: 0.8662\n",
      "Epoch 15/100\n",
      "5940/5940 [==============================] - 4s 605us/step - loss: 0.3154 - acc: 0.8586 - val_loss: 0.3089 - val_acc: 0.8652\n",
      "Epoch 16/100\n",
      "5940/5940 [==============================] - 4s 592us/step - loss: 0.3145 - acc: 0.8577 - val_loss: 0.3074 - val_acc: 0.8657\n",
      "Epoch 17/100\n",
      "5940/5940 [==============================] - 4s 614us/step - loss: 0.3133 - acc: 0.8596 - val_loss: 0.3058 - val_acc: 0.8652\n",
      "Epoch 18/100\n",
      "5940/5940 [==============================] - 4s 652us/step - loss: 0.3122 - acc: 0.8599 - val_loss: 0.3044 - val_acc: 0.8652\n",
      "Epoch 19/100\n",
      "5940/5940 [==============================] - 4s 621us/step - loss: 0.3109 - acc: 0.8598 - val_loss: 0.3019 - val_acc: 0.8672\n",
      "Epoch 20/100\n",
      "5940/5940 [==============================] - 3s 587us/step - loss: 0.3092 - acc: 0.8628 - val_loss: 0.3043 - val_acc: 0.8641\n",
      "Epoch 21/100\n",
      "5940/5940 [==============================] - 4s 589us/step - loss: 0.3089 - acc: 0.8631 - val_loss: 0.3011 - val_acc: 0.8652\n",
      "Epoch 22/100\n",
      "5940/5940 [==============================] - 4s 645us/step - loss: 0.3071 - acc: 0.8626 - val_loss: 0.2989 - val_acc: 0.8672\n",
      "Epoch 23/100\n",
      "5940/5940 [==============================] - 4s 656us/step - loss: 0.3063 - acc: 0.8626 - val_loss: 0.2985 - val_acc: 0.8672\n",
      "Epoch 24/100\n",
      "5940/5940 [==============================] - 4s 589us/step - loss: 0.3052 - acc: 0.8640 - val_loss: 0.2964 - val_acc: 0.8677\n",
      "Epoch 25/100\n",
      "5940/5940 [==============================] - 3s 587us/step - loss: 0.3041 - acc: 0.8643 - val_loss: 0.2942 - val_acc: 0.8682\n",
      "Epoch 26/100\n",
      "5940/5940 [==============================] - 4s 628us/step - loss: 0.3033 - acc: 0.8630 - val_loss: 0.2938 - val_acc: 0.8677\n",
      "Epoch 27/100\n",
      "5940/5940 [==============================] - 4s 647us/step - loss: 0.3023 - acc: 0.8635 - val_loss: 0.2919 - val_acc: 0.8707\n",
      "Epoch 28/100\n",
      "5940/5940 [==============================] - 4s 622us/step - loss: 0.3010 - acc: 0.8640 - val_loss: 0.2877 - val_acc: 0.8753\n",
      "Epoch 29/100\n",
      "5940/5940 [==============================] - 3s 587us/step - loss: 0.3002 - acc: 0.8646 - val_loss: 0.2880 - val_acc: 0.8737\n",
      "Epoch 30/100\n",
      "5940/5940 [==============================] - 4s 626us/step - loss: 0.2989 - acc: 0.8668 - val_loss: 0.2873 - val_acc: 0.8742\n",
      "Epoch 31/100\n",
      "5940/5940 [==============================] - 4s 634us/step - loss: 0.2986 - acc: 0.8662 - val_loss: 0.2887 - val_acc: 0.8717\n",
      "Epoch 32/100\n",
      "5940/5940 [==============================] - 4s 756us/step - loss: 0.2977 - acc: 0.8672 - val_loss: 0.2844 - val_acc: 0.8763\n",
      "Epoch 33/100\n",
      "5940/5940 [==============================] - 4s 679us/step - loss: 0.2964 - acc: 0.8678 - val_loss: 0.2855 - val_acc: 0.8742\n",
      "Epoch 34/100\n",
      "5940/5940 [==============================] - 4s 723us/step - loss: 0.2955 - acc: 0.8668 - val_loss: 0.2830 - val_acc: 0.8763\n",
      "Epoch 35/100\n",
      "5940/5940 [==============================] - 4s 744us/step - loss: 0.2957 - acc: 0.8677 - val_loss: 0.2826 - val_acc: 0.8768\n",
      "Epoch 36/100\n",
      "5940/5940 [==============================] - 4s 692us/step - loss: 0.2934 - acc: 0.8672 - val_loss: 0.2822 - val_acc: 0.8758\n",
      "Epoch 37/100\n",
      "5940/5940 [==============================] - 4s 595us/step - loss: 0.2933 - acc: 0.8689 - val_loss: 0.2815 - val_acc: 0.8763\n",
      "Epoch 38/100\n",
      "5940/5940 [==============================] - 4s 628us/step - loss: 0.2908 - acc: 0.8707 - val_loss: 0.2797 - val_acc: 0.8768\n",
      "Epoch 39/100\n",
      "5940/5940 [==============================] - 4s 637us/step - loss: 0.2911 - acc: 0.8690 - val_loss: 0.2799 - val_acc: 0.8768\n",
      "Epoch 40/100\n",
      "5940/5940 [==============================] - 4s 643us/step - loss: 0.2907 - acc: 0.8690 - val_loss: 0.2787 - val_acc: 0.8788\n",
      "Epoch 41/100\n",
      "5940/5940 [==============================] - 4s 592us/step - loss: 0.2888 - acc: 0.8705 - val_loss: 0.2782 - val_acc: 0.8788\n",
      "Epoch 42/100\n",
      "5940/5940 [==============================] - 4s 605us/step - loss: 0.2888 - acc: 0.8717 - val_loss: 0.2771 - val_acc: 0.8793\n",
      "Epoch 43/100\n",
      "5940/5940 [==============================] - 4s 656us/step - loss: 0.2915 - acc: 0.8697 - val_loss: 0.2766 - val_acc: 0.8813\n",
      "Epoch 44/100\n",
      "5940/5940 [==============================] - 4s 643us/step - loss: 0.2862 - acc: 0.8734 - val_loss: 0.2768 - val_acc: 0.8788\n",
      "Epoch 45/100\n",
      "5940/5940 [==============================] - 4s 595us/step - loss: 0.2857 - acc: 0.8719 - val_loss: 0.2755 - val_acc: 0.8793\n",
      "Epoch 46/100\n",
      "5940/5940 [==============================] - 4s 595us/step - loss: 0.2857 - acc: 0.8717 - val_loss: 0.2755 - val_acc: 0.8798\n",
      "Epoch 47/100\n",
      "5940/5940 [==============================] - 4s 638us/step - loss: 0.2888 - acc: 0.8709 - val_loss: 0.2745 - val_acc: 0.8808\n",
      "Epoch 48/100\n",
      "5940/5940 [==============================] - 4s 639us/step - loss: 0.2841 - acc: 0.8729 - val_loss: 0.2747 - val_acc: 0.8843\n",
      "Epoch 49/100\n",
      "5940/5940 [==============================] - 4s 610us/step - loss: 0.2820 - acc: 0.8734 - val_loss: 0.2738 - val_acc: 0.8798\n",
      "Epoch 50/100\n",
      "5940/5940 [==============================] - 4s 589us/step - loss: 0.2816 - acc: 0.8741 - val_loss: 0.2732 - val_acc: 0.8793\n",
      "Epoch 51/100\n",
      "5940/5940 [==============================] - 4s 621us/step - loss: 0.2850 - acc: 0.8737 - val_loss: 0.2729 - val_acc: 0.8798\n",
      "Epoch 52/100\n",
      "5940/5940 [==============================] - 4s 651us/step - loss: 0.2878 - acc: 0.8727 - val_loss: 0.2729 - val_acc: 0.8793\n",
      "Epoch 53/100\n",
      "5940/5940 [==============================] - 4s 631us/step - loss: 0.2773 - acc: 0.8763 - val_loss: 0.2719 - val_acc: 0.8813\n",
      "Epoch 54/100\n",
      "5940/5940 [==============================] - 4s 597us/step - loss: 0.2807 - acc: 0.8758 - val_loss: 0.2713 - val_acc: 0.8813\n",
      "Epoch 55/100\n",
      "5940/5940 [==============================] - 4s 608us/step - loss: 0.2818 - acc: 0.8741 - val_loss: 0.2711 - val_acc: 0.8808\n",
      "Epoch 56/100\n",
      "5940/5940 [==============================] - 4s 645us/step - loss: 0.2793 - acc: 0.8756 - val_loss: 0.2704 - val_acc: 0.8848\n",
      "Epoch 57/100\n",
      "5940/5940 [==============================] - 5s 772us/step - loss: 0.2818 - acc: 0.8744 - val_loss: 0.2701 - val_acc: 0.8823\n",
      "Epoch 58/100\n",
      "5940/5940 [==============================] - 4s 728us/step - loss: 0.2798 - acc: 0.8761 - val_loss: 0.2696 - val_acc: 0.8848\n",
      "Epoch 59/100\n",
      "5940/5940 [==============================] - 4s 747us/step - loss: 0.2814 - acc: 0.8763 - val_loss: 0.2694 - val_acc: 0.8848\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5940/5940 [==============================] - 4s 647us/step - loss: 0.2843 - acc: 0.8749 - val_loss: 0.2704 - val_acc: 0.8823\n",
      "Epoch 61/100\n",
      "5940/5940 [==============================] - 4s 629us/step - loss: 0.2727 - acc: 0.8778 - val_loss: 0.2769 - val_acc: 0.8803\n",
      "Epoch 62/100\n",
      "5940/5940 [==============================] - 3s 585us/step - loss: 0.2756 - acc: 0.8778 - val_loss: 0.2713 - val_acc: 0.8808\n",
      "Epoch 63/100\n",
      "5940/5940 [==============================] - 4s 589us/step - loss: 0.2762 - acc: 0.8773 - val_loss: 0.2684 - val_acc: 0.8838\n",
      "Epoch 64/100\n",
      "5940/5940 [==============================] - 4s 660us/step - loss: 0.2795 - acc: 0.8756 - val_loss: 0.2718 - val_acc: 0.8803\n",
      "Epoch 65/100\n",
      "5940/5940 [==============================] - 4s 659us/step - loss: 0.2777 - acc: 0.8778 - val_loss: 0.2684 - val_acc: 0.8838\n",
      "Epoch 66/100\n",
      "5940/5940 [==============================] - 3s 584us/step - loss: 0.2751 - acc: 0.8788 - val_loss: 0.2697 - val_acc: 0.8828\n",
      "Epoch 67/100\n",
      "5940/5940 [==============================] - 4s 589us/step - loss: 0.2762 - acc: 0.8785 - val_loss: 0.2767 - val_acc: 0.8793\n",
      "Epoch 68/100\n",
      "5940/5940 [==============================] - 4s 642us/step - loss: 0.2753 - acc: 0.8806 - val_loss: 0.2765 - val_acc: 0.8798\n",
      "Epoch 69/100\n",
      "5940/5940 [==============================] - 4s 661us/step - loss: 0.2740 - acc: 0.8816 - val_loss: 0.2792 - val_acc: 0.8763\n",
      "Epoch 70/100\n",
      "5940/5940 [==============================] - 4s 620us/step - loss: 0.2866 - acc: 0.8715 - val_loss: 0.2666 - val_acc: 0.8889\n",
      "Epoch 71/100\n",
      "5940/5940 [==============================] - 4s 591us/step - loss: 0.2722 - acc: 0.8798 - val_loss: 0.2766 - val_acc: 0.8793\n",
      "Epoch 72/100\n",
      "5940/5940 [==============================] - 4s 637us/step - loss: 0.2753 - acc: 0.8805 - val_loss: 0.2785 - val_acc: 0.8788\n",
      "Epoch 73/100\n",
      "5940/5940 [==============================] - 4s 646us/step - loss: 0.2746 - acc: 0.8786 - val_loss: 0.2891 - val_acc: 0.8727\n",
      "Epoch 74/100\n",
      "5940/5940 [==============================] - 4s 660us/step - loss: 0.2753 - acc: 0.8764 - val_loss: 0.2776 - val_acc: 0.8783\n",
      "Epoch 75/100\n",
      "5940/5940 [==============================] - 4s 610us/step - loss: 0.2733 - acc: 0.8800 - val_loss: 0.2884 - val_acc: 0.8742\n",
      "Epoch 76/100\n",
      "5940/5940 [==============================] - 4s 613us/step - loss: 0.2745 - acc: 0.8779 - val_loss: 0.2775 - val_acc: 0.8783\n",
      "Epoch 77/100\n",
      "5940/5940 [==============================] - 4s 660us/step - loss: 0.2706 - acc: 0.8803 - val_loss: 0.2907 - val_acc: 0.8732\n",
      "Epoch 78/100\n",
      "5940/5940 [==============================] - 4s 664us/step - loss: 0.2715 - acc: 0.8788 - val_loss: 0.2882 - val_acc: 0.8732\n",
      "Epoch 79/100\n",
      "5940/5940 [==============================] - 4s 592us/step - loss: 0.2754 - acc: 0.8768 - val_loss: 0.2767 - val_acc: 0.8778\n",
      "Epoch 80/100\n",
      "5940/5940 [==============================] - 3s 587us/step - loss: 0.2703 - acc: 0.8818 - val_loss: 0.2836 - val_acc: 0.8768\n",
      "Epoch 81/100\n",
      "5940/5940 [==============================] - 4s 646us/step - loss: 0.2697 - acc: 0.8798 - val_loss: 0.2940 - val_acc: 0.8717\n",
      "Epoch 82/100\n",
      "5940/5940 [==============================] - 4s 654us/step - loss: 0.2664 - acc: 0.8818 - val_loss: 0.2956 - val_acc: 0.8712\n",
      "Epoch 83/100\n",
      "5940/5940 [==============================] - 4s 687us/step - loss: 0.2695 - acc: 0.8835 - val_loss: 0.2662 - val_acc: 0.8859\n",
      "Epoch 84/100\n",
      "5940/5940 [==============================] - 4s 705us/step - loss: 0.2689 - acc: 0.8818 - val_loss: 0.3508 - val_acc: 0.8419\n",
      "Epoch 85/100\n",
      "5940/5940 [==============================] - 4s 684us/step - loss: 0.2723 - acc: 0.8823 - val_loss: 0.3211 - val_acc: 0.8566\n",
      "Epoch 86/100\n",
      "5940/5940 [==============================] - 4s 668us/step - loss: 0.2702 - acc: 0.8827 - val_loss: 0.3042 - val_acc: 0.8657\n",
      "Epoch 87/100\n",
      "5940/5940 [==============================] - 4s 714us/step - loss: 0.2692 - acc: 0.8813 - val_loss: 0.3411 - val_acc: 0.8460\n",
      "Epoch 88/100\n",
      "5940/5940 [==============================] - 4s 708us/step - loss: 0.2644 - acc: 0.8835 - val_loss: 0.3834 - val_acc: 0.8298\n",
      "Epoch 89/100\n",
      "5940/5940 [==============================] - 5s 775us/step - loss: 0.2676 - acc: 0.8830 - val_loss: 0.3560 - val_acc: 0.8348\n",
      "Epoch 90/100\n",
      "5940/5940 [==============================] - 4s 729us/step - loss: 0.2692 - acc: 0.8833 - val_loss: 0.3512 - val_acc: 0.8439\n",
      "Epoch 91/100\n",
      "5940/5940 [==============================] - 4s 679us/step - loss: 0.2759 - acc: 0.8769 - val_loss: 0.2763 - val_acc: 0.8813\n",
      "Epoch 92/100\n",
      "5940/5940 [==============================] - 4s 718us/step - loss: 0.2638 - acc: 0.8860 - val_loss: 0.3788 - val_acc: 0.8313\n",
      "Epoch 93/100\n",
      "5940/5940 [==============================] - 4s 754us/step - loss: 0.2772 - acc: 0.8806 - val_loss: 0.2639 - val_acc: 0.8864\n",
      "Epoch 94/100\n",
      "5940/5940 [==============================] - 4s 733us/step - loss: 0.2604 - acc: 0.8855 - val_loss: 0.3476 - val_acc: 0.8439\n",
      "Epoch 95/100\n",
      "5940/5940 [==============================] - 4s 705us/step - loss: 0.2720 - acc: 0.8791 - val_loss: 0.2827 - val_acc: 0.8803\n",
      "Epoch 96/100\n",
      "5940/5940 [==============================] - 4s 734us/step - loss: 0.2615 - acc: 0.8860 - val_loss: 0.3524 - val_acc: 0.8465\n",
      "Epoch 97/100\n",
      "5940/5940 [==============================] - 4s 729us/step - loss: 0.2661 - acc: 0.8843 - val_loss: 0.3721 - val_acc: 0.8308\n",
      "Epoch 98/100\n",
      "5940/5940 [==============================] - 4s 718us/step - loss: 0.2675 - acc: 0.8837 - val_loss: 0.2760 - val_acc: 0.8843\n",
      "Epoch 99/100\n",
      "5940/5940 [==============================] - 4s 684us/step - loss: 0.2595 - acc: 0.8864 - val_loss: 0.3562 - val_acc: 0.8444\n",
      "Epoch 100/100\n",
      "5940/5940 [==============================] - 5s 761us/step - loss: 0.2605 - acc: 0.8852 - val_loss: 0.2639 - val_acc: 0.8919\n",
      "5940/5940 [==============================] - 2s 289us/step\n",
      "Detail of training losses and accuracies:  [0.2371006815822839, 0.8968013468414846]\n",
      "Testing accuracy: 0.8919191919191919\n",
      "f1 score: 0.8047445255474452\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(units= 100, activation= 'relu', input_dim= X_train_count.shape[1], kernel_initializer= 'uniform'))\n",
    "model.add(Dense(units= 50, activation= 'relu'))\n",
    "model.add(Dense(units= 20, activation= 'sigmoid'))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "sgd= SGD(lr= 0.1)\n",
    "\n",
    "model.compile(optimizer= sgd, loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x= X_train_count, y= y_train_c, batch_size= 128, epochs= 100, class_weight= 'balanced', shuffle= False,\n",
    "          validation_data= (X_test_count, y_test_c))\n",
    "\n",
    "y_pred_c= model.predict_classes(X_test_count, batch_size= 128)\n",
    "\n",
    "#print accuracies\n",
    "train_acc= model.evaluate(X_train_count, y_train_c, batch_size= 128)\n",
    "test_acc= accuracy_score(y_test_c, y_pred_c)\n",
    "f1= f1_score(y_test_c, y_pred_c)\n",
    "\n",
    "print('Detail of training losses and accuracies: ', train_acc)\n",
    "print('Testing accuracy: {}' .format(test_acc))\n",
    "print('f1 score: {}' .format(f1))\n",
    "\n",
    "y_pred_real_c= model.predict_classes(test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With two hidden layers and adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5940 samples, validate on 1980 samples\n",
      "Epoch 1/5\n",
      "5940/5940 [==============================] - 4s 700us/step - loss: 0.4991 - acc: 0.7407 - val_loss: 0.3440 - val_acc: 0.8682\n",
      "Epoch 2/5\n",
      "5940/5940 [==============================] - 4s 619us/step - loss: 0.2856 - acc: 0.8806 - val_loss: 0.2766 - val_acc: 0.8848\n",
      "Epoch 3/5\n",
      "5940/5940 [==============================] - 4s 634us/step - loss: 0.2193 - acc: 0.9101 - val_loss: 0.2576 - val_acc: 0.8975\n",
      "Epoch 4/5\n",
      "5940/5940 [==============================] - 3s 589us/step - loss: 0.1650 - acc: 0.9421 - val_loss: 0.2516 - val_acc: 0.9010\n",
      "Epoch 5/5\n",
      "5940/5940 [==============================] - 4s 595us/step - loss: 0.1190 - acc: 0.9614 - val_loss: 0.2586 - val_acc: 0.9025\n",
      "5940/5940 [==============================] - 1s 189us/step\n",
      "Detail of training losses and accuracies:  [0.09593365830102754, 0.9705387205387206]\n",
      "Testing accuracy: 0.9025252525252525\n",
      "f1 score: 0.8194574368568756\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(units= 50, activation= 'relu', input_dim= X_train_count.shape[1], kernel_initializer= 'uniform'))\n",
    "model.add(Dense(units= 20, activation= 'relu'))\n",
    "#model.add(Dense(units= 20, activation= 'relu'))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "#sgd= SGD(lr= 0.1)\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x= X_train_count, y= y_train_c, batch_size= 128, epochs= 5, class_weight= 'balanced', shuffle= False,\n",
    "          validation_data= (X_test_count, y_test_c))\n",
    "\n",
    "y_pred_c= model.predict_classes(X_test_count, batch_size= 128)\n",
    "\n",
    "#print accuracies\n",
    "train_acc= model.evaluate(X_train_count, y_train_c, batch_size= 128)\n",
    "test_acc= accuracy_score(y_test_c, y_pred_c)\n",
    "f1= f1_score(y_test_c, y_pred_c)\n",
    "\n",
    "print('Detail of training losses and accuracies: ', train_acc)\n",
    "print('Testing accuracy: {}' .format(test_acc))\n",
    "print('f1 score: {}' .format(f1))\n",
    "\n",
    "y_pred_real_c_2= model.predict_classes(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5940 samples, validate on 1980 samples\n",
      "Epoch 1/5\n",
      "5940/5940 [==============================] - 6s 1ms/step - loss: 0.5049 - acc: 0.7254 - val_loss: 0.3579 - val_acc: 0.8490\n",
      "Epoch 2/5\n",
      "5940/5940 [==============================] - 6s 936us/step - loss: 0.2920 - acc: 0.8739 - val_loss: 0.2745 - val_acc: 0.8833\n",
      "Epoch 3/5\n",
      "5940/5940 [==============================] - 5s 853us/step - loss: 0.2237 - acc: 0.9098 - val_loss: 0.2544 - val_acc: 0.8970\n",
      "Epoch 4/5\n",
      "5940/5940 [==============================] - 5s 853us/step - loss: 0.1704 - acc: 0.9386 - val_loss: 0.2451 - val_acc: 0.9015\n",
      "Epoch 5/5\n",
      "5940/5940 [==============================] - 5s 887us/step - loss: 0.1245 - acc: 0.9613 - val_loss: 0.2496 - val_acc: 0.9025\n",
      "5940/5940 [==============================] - 1s 238us/step\n",
      "Detail of training losses and accuracies:  [0.10107676187398458, 0.97003367003367]\n",
      "Testing accuracy: 0.9025252525252525\n",
      "f1 score: 0.8187793427230047\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(units= 100, activation= 'relu', input_dim= X_train_count.shape[1], kernel_initializer= 'uniform'))\n",
    "model.add(Dense(units= 10, activation= 'relu'))\n",
    "#model.add(Dense(units= 20, activation= 'relu'))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "#sgd= SGD(lr= 0.1)\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x= X_train_count, y= y_train_c, batch_size= 128, epochs= 5, class_weight= 'balanced', shuffle= False,\n",
    "          validation_data= (X_test_count, y_test_c))\n",
    "\n",
    "y_pred_c= model.predict_classes(X_test_count, batch_size= 128)\n",
    "\n",
    "#print accuracies\n",
    "train_acc= model.evaluate(X_train_count, y_train_c, batch_size= 128)\n",
    "test_acc= accuracy_score(y_test_c, y_pred_c)\n",
    "f1= f1_score(y_test_c, y_pred_c)\n",
    "\n",
    "print('Detail of training losses and accuracies: ', train_acc)\n",
    "print('Testing accuracy: {}' .format(test_acc))\n",
    "print('f1 score: {}' .format(f1))\n",
    "\n",
    "y_pred_real_c_2= model.predict_classes(test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf= csr_matrix(X_train_tfidf)\n",
    "X_test_tfidf= csr_matrix(X_test_tfidf)\n",
    "test_tfidf= csr_matrix(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5940 samples, validate on 1980 samples\n",
      "Epoch 1/15\n",
      "5940/5940 [==============================] - 3s 479us/step - loss: 0.4140 - acc: 0.7919 - val_loss: 0.3110 - val_acc: 0.8626\n",
      "Epoch 2/15\n",
      "5940/5940 [==============================] - 1s 174us/step - loss: 0.3073 - acc: 0.8658 - val_loss: 0.2946 - val_acc: 0.8677\n",
      "Epoch 3/15\n",
      "5940/5940 [==============================] - 1s 187us/step - loss: 0.2689 - acc: 0.8887 - val_loss: 0.2706 - val_acc: 0.8914\n",
      "Epoch 4/15\n",
      "5940/5940 [==============================] - 1s 174us/step - loss: 0.2246 - acc: 0.9113 - val_loss: 0.2659 - val_acc: 0.8722\n",
      "Epoch 5/15\n",
      "5940/5940 [==============================] - 1s 197us/step - loss: 0.1830 - acc: 0.9269 - val_loss: 0.2855 - val_acc: 0.8869\n",
      "Epoch 6/15\n",
      "5940/5940 [==============================] - 1s 174us/step - loss: 0.1793 - acc: 0.9251 - val_loss: 0.2646 - val_acc: 0.8838\n",
      "Epoch 7/15\n",
      "5940/5940 [==============================] - 1s 176us/step - loss: 0.1471 - acc: 0.9441 - val_loss: 0.2609 - val_acc: 0.8929\n",
      "Epoch 8/15\n",
      "5940/5940 [==============================] - 1s 192us/step - loss: 0.1341 - acc: 0.9476 - val_loss: 0.2752 - val_acc: 0.8874\n",
      "Epoch 9/15\n",
      "5940/5940 [==============================] - 1s 192us/step - loss: 0.1181 - acc: 0.9562 - val_loss: 0.2954 - val_acc: 0.8833\n",
      "Epoch 10/15\n",
      "5940/5940 [==============================] - 1s 176us/step - loss: 0.1055 - acc: 0.9611 - val_loss: 0.3197 - val_acc: 0.8783\n",
      "Epoch 11/15\n",
      "5940/5940 [==============================] - 1s 218us/step - loss: 0.0952 - acc: 0.9646 - val_loss: 0.3408 - val_acc: 0.8758\n",
      "Epoch 12/15\n",
      "5940/5940 [==============================] - 2s 265us/step - loss: 0.0857 - acc: 0.9724 - val_loss: 0.3685 - val_acc: 0.8768\n",
      "Epoch 13/15\n",
      "5940/5940 [==============================] - 2s 325us/step - loss: 0.0794 - acc: 0.9739 - val_loss: 0.3941 - val_acc: 0.8758\n",
      "Epoch 14/15\n",
      "5940/5940 [==============================] - 1s 200us/step - loss: 0.0769 - acc: 0.9727 - val_loss: 0.4067 - val_acc: 0.8823\n",
      "Epoch 15/15\n",
      "5940/5940 [==============================] - 1s 211us/step - loss: 0.0684 - acc: 0.9773 - val_loss: 0.4203 - val_acc: 0.8758\n",
      "5940/5940 [==============================] - 0s 68us/step\n",
      "Detail of training losses and accuracies:  [0.053460718861934715, 0.9873737373737373]\n",
      "Testing accuracy: 0.8757575757575757\n",
      "f1 score: 0.7592954990215263\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(units= 100, activation= 'relu', input_dim= X_train_tfidf.shape[1], kernel_initializer= 'uniform'))\n",
    "model.add(Dense(units= 50, activation= 'relu'))\n",
    "model.add(Dense(units= 20, activation= 'relu'))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "#sgd= SGD(lr= 0.1)\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x= X_train_tfidf, y= y_train_t, batch_size= 128, epochs= 15, class_weight= 'balanced', shuffle= True,\n",
    "          validation_data= (X_test_tfidf, y_test_t))\n",
    "\n",
    "y_pred_t= model.predict_classes(X_test_tfidf, batch_size= 128)\n",
    "\n",
    "#print accuracies\n",
    "train_acc= model.evaluate(X_train_tfidf, y_train_t, batch_size= 128)\n",
    "test_acc= accuracy_score(y_test_t, y_pred_t)\n",
    "f1= f1_score(y_test_t, y_pred_t)\n",
    "\n",
    "print('Detail of training losses and accuracies: ', train_acc)\n",
    "print('Testing accuracy: {}' .format(test_acc))\n",
    "print('f1 score: {}' .format(f1))\n",
    "\n",
    "y_pred_real_1= model.predict_classes(test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5940/5940 [==============================] - 6s 1ms/step - loss: 0.3571 - acc: 0.8322\n",
      "Epoch 2/10\n",
      "5940/5940 [==============================] - 5s 788us/step - loss: 0.2223 - acc: 0.9111\n",
      "Epoch 3/10\n",
      "5940/5940 [==============================] - 5s 762us/step - loss: 0.1428 - acc: 0.9520\n",
      "Epoch 4/10\n",
      "5940/5940 [==============================] - 5s 837us/step - loss: 0.0806 - acc: 0.9771\n",
      "Epoch 5/10\n",
      "5940/5940 [==============================] - 5s 808us/step - loss: 0.0477 - acc: 0.9870\n",
      "Epoch 6/10\n",
      "5940/5940 [==============================] - 5s 784us/step - loss: 0.0263 - acc: 0.9944\n",
      "Epoch 7/10\n",
      "5940/5940 [==============================] - 5s 823us/step - loss: 0.0173 - acc: 0.9971\n",
      "Epoch 8/10\n",
      "5940/5940 [==============================] - 5s 835us/step - loss: 0.0119 - acc: 0.9978\n",
      "Epoch 9/10\n",
      "5940/5940 [==============================] - 5s 788us/step - loss: 0.0076 - acc: 0.9992 2s - los - ETA: 0s - loss: 0.0077 - acc: 0.99\n",
      "Epoch 10/10\n",
      "5940/5940 [==============================] - 5s 851us/step - loss: 0.0045 - acc: 0.9993\n",
      "5940/5940 [==============================] - 2s 318us/step\n",
      "Detail of training losses and accuracies:  [0.0026310378710679735, 0.9998316498316498]\n",
      "Testing accuracy: 0.8792929292929293\n",
      "f1 score: 0.7553735926305015\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(units= 100, activation= 'relu', input_dim= X_train_count.shape[1], kernel_initializer= 'uniform'))\n",
    "model.add(Dense(units= 50, activation= 'relu'))\n",
    "model.add(Dense(units= 20, activation= 'relu'))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "#sgd= SGD(lr= 0.1)\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "model.fit(x= X_train_count, y= y_train_c, batch_size= 128, epochs= 10, shuffle= True)#, validation_data= (X_test_count_1, y_test_c))\n",
    "\n",
    "y_pred_c= model.predict_classes(X_test_count, batch_size= 128)\n",
    "\n",
    "#print accuracies\n",
    "train_acc= model.evaluate(X_train_count, y_train_c, batch_size= 128)\n",
    "test_acc= accuracy_score(y_test_c, y_pred_c)\n",
    "f1= f1_score(y_test_c, y_pred_c)\n",
    "\n",
    "print('Detail of training losses and accuracies: ', train_acc)\n",
    "print('Testing accuracy: {}' .format(test_acc))\n",
    "print('f1 score: {}' .format(f1))\n",
    "\n",
    "y_pred_real= model.predict_classes(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5940, 18026), (1953, 22158))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape, test_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5940 samples, validate on 1980 samples\n",
      "Epoch 1/5\n",
      "5940/5940 [==============================] - 3s 563us/step - loss: 0.3893 - acc: 0.8157 - val_loss: 0.3074 - val_acc: 0.8591\n",
      "Epoch 2/5\n",
      "5940/5940 [==============================] - 2s 355us/step - loss: 0.3012 - acc: 0.8663 - val_loss: 0.2896 - val_acc: 0.8768\n",
      "Epoch 3/5\n",
      "5940/5940 [==============================] - 2s 368us/step - loss: 0.2742 - acc: 0.8845 - val_loss: 0.2666 - val_acc: 0.8859\n",
      "Epoch 4/5\n",
      "5940/5940 [==============================] - 2s 362us/step - loss: 0.2402 - acc: 0.9037 - val_loss: 0.2488 - val_acc: 0.8914\n",
      "Epoch 5/5\n",
      "5940/5940 [==============================] - 2s 318us/step - loss: 0.2022 - acc: 0.9184 - val_loss: 0.2401 - val_acc: 0.8995\n",
      "5940/5940 [==============================] - 1s 89us/step\n",
      "[0.1683602602995606, 0.9368686866679978]\n",
      "0.8994949494949495\n",
      "0.8131455399061033\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(Dense(units= 200, activation= 'relu', input_dim= X_train_tfidf_.shape[1]))\n",
    "model.add(Dropout(0.1, seed= 0))\n",
    "model.add(Dense(units= 20, activation= 'relu'))\n",
    "model.add(Dropout(0.1, seed= 0))\n",
    "model.add(Dense(units= 1, activation= 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer= 'adam', loss= 'binary_crossentropy', metrics= ['accuracy'])\n",
    "\n",
    "model.fit(X_train_tfidf_, y_train_t_, batch_size= 200, epochs= 5, validation_data= (X_test_tfidf_, y_test_t_))\n",
    "\n",
    "y_pred= model.predict_classes(X_test_tfidf_, batch_size= 200)\n",
    "\n",
    "train_acc= model.evaluate(X_train_tfidf_, y_train_t_, batch_size= 200)\n",
    "\n",
    "test_acc= accuracy_score(y_test_t_, y_pred)\n",
    "\n",
    "f1= f1_score(y_test_t_, y_pred)\n",
    "\n",
    "print(train_acc)\n",
    "print(test_acc)\n",
    "print(f1)\n",
    "\n",
    "y_pred_real_tf= model.predict_classes(test_tfidf_, batch_size= 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used ensmble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, VotingClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_bag_log= BaggingClassifier(clf_log, random_state= 0, max_samples= 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 18820)\n",
      "X_test_vect (1980, 18820)\n",
      "test_vect (1953, 18820)\n",
      "Training accuracy: 0.9488215488215488\n",
      "Testing accuracy: 0.8964646464646465\n",
      "f1 score: 0.8138056312443233\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.90      0.93      1475\n",
      "        1.0       0.75      0.89      0.81       505\n",
      "\n",
      "avg / total       0.91      0.90      0.90      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_5, pred_5= ml_modeling(clf_bag_log, train, train['label'], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ada_log= AdaBoostClassifier(clf_log, learning_rate= 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 18820)\n",
      "X_test_vect (1980, 18820)\n",
      "test_vect (1953, 18820)\n",
      "Training accuracy: 0.8604377104377104\n",
      "Testing accuracy: 0.8641414141414141\n",
      "f1 score: 0.7726120033812343\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.85      0.90      1475\n",
      "        1.0       0.67      0.90      0.77       505\n",
      "\n",
      "avg / total       0.89      0.86      0.87      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_6, pred_6= ml_modeling(clf_ada_log, train, train['label'], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_vote= VotingClassifier([('log', clf_log), ('bag_log', clf_bag_log)], voting= 'soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 18820)\n",
      "X_test_vect (1980, 18820)\n",
      "test_vect (1953, 18820)\n",
      "Training accuracy: 0.9631313131313132\n",
      "Testing accuracy: 0.8939393939393939\n",
      "f1 score: 0.8073394495412844\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.95      0.90      0.93      1475\n",
      "        1.0       0.75      0.87      0.81       505\n",
      "\n",
      "avg / total       0.90      0.89      0.90      1980\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sngupta\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\sngupta\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "C:\\Users\\sngupta\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "model_7, pred_7= ml_modeling(clf_vote, train, train['label'], test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_modeling(model, train, target, test, countVectorizer= True, tfidfVectorizer= False, col_to_add= col_to_add):\n",
    "    \n",
    "    X_train, X_test, y_train, y_test= train_test_split(train, target, random_state= 5)\n",
    "    \n",
    "    if countVectorizer:\n",
    "        vect= CountVectorizer().fit(X_train['tweets'])\n",
    "        X_train_vect= vect.transform(X_train['tweets'])\n",
    "        X_test_vect= vect.transform(X_test['tweets'])\n",
    "        test_vect= vect.transform(test['tweets'])\n",
    "        \n",
    "        #now add the columns to the sparse matrix using the scipy library\n",
    "        for col in col_to_add:\n",
    "            X_train_vect= hstack((X_train_vect, np.array(X_train[col])[:, None]))\n",
    "            X_test_vect= hstack((X_test_vect, np.array(X_test[col])[:, None]))\n",
    "            test_vect= hstack((test_vect, np.array(test[col])[:, None]))\n",
    "        \n",
    "        print('X_train_vect', X_train_vect.shape)\n",
    "        print('X_test_vect', X_test_vect.shape)\n",
    "        print('test_vect', test_vect.shape)\n",
    "        \n",
    "        #modeling and prediction\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        prediction= model.predict(X_test_vect)\n",
    "        \n",
    "        #print accuracies\n",
    "        train_acc= model.score(X_train_vect, y_train)\n",
    "        test_acc= accuracy_score(y_test, prediction)\n",
    "        f1= f1_score(y_test, prediction)\n",
    "        \n",
    "        print('Training accuracy: {}' .format(train_acc))\n",
    "        print('Testing accuracy: {}' .format(test_acc))\n",
    "        print('f1 score: {}' .format(f1))\n",
    "        \n",
    "        print('Classification Report: ')\n",
    "        print(classification_report(y_test, prediction))\n",
    "        \n",
    "    if tfidfVectorizer:\n",
    "        vect= TfidfVectorizer(min_df= 5, ngram_range= (1, 3)).fit(X_train['tweets'])\n",
    "        X_train_vect= vect.transform(X_train['tweets'])\n",
    "        X_test_vect= vect.transform(X_test['tweets'])\n",
    "        test_vect= vect.transform(test['tweets'])\n",
    "        \n",
    "        #now add the columns to the sparse matrix using the scipy library\n",
    "        for col in col_to_add:\n",
    "            X_train_vect= hstack((X_train_vect, np.array(X_train[col])[:, None]))\n",
    "            X_test_vect= hstack((X_test_vect, np.array(X_test[col])[:, None]))\n",
    "            test_vect= hstack((test_vect, np.array(test[col])[:, None]))\n",
    "        \n",
    "        print('X_train_vect', X_train_vect.shape)\n",
    "        print('X_test_vect', X_test_vect.shape)\n",
    "        print('test_vect', test_vect.shape)\n",
    "        \n",
    "        #modeling and prediction\n",
    "        model.fit(X_train_vect, y_train)\n",
    "        prediction= model.predict(X_test_vect)\n",
    "        \n",
    "        #print accuracies\n",
    "        train_acc= model.score(X_train_vect, y_train)\n",
    "        test_acc= accuracy_score(y_test, prediction)\n",
    "        f1= f1_score(y_test, prediction)\n",
    "        \n",
    "        print('Training accuracy: {}' .format(train_acc))\n",
    "        print('Testing accuracy: {}' .format(test_acc))\n",
    "        print('f1 score: {}' .format(f1))\n",
    "        \n",
    "        print('Classification Report: ')\n",
    "        print(classification_report(y_test, prediction))\n",
    "        \n",
    "    return model, model.predict(test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_vect (5940, 18026)\n",
      "X_test_vect (1980, 18026)\n",
      "test_vect (1953, 18026)\n",
      "Training accuracy: 0.9484848484848485\n",
      "Testing accuracy: 0.8984848484848484\n",
      "f1 score: 0.8161024702653248\n",
      "Classification Report: \n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.96      0.90      0.93      1475\n",
      "        1.0       0.76      0.88      0.82       505\n",
      "\n",
      "avg / total       0.91      0.90      0.90      1980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_10, pred_10= ml_modeling(clf_bag_log, train, train['label'], test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, I got the best result using deep learning with f1-score 0.81945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
